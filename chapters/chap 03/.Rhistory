tune_grid <- expand.grid(min.node.size = c(5, 10, 15),
sample.fraction = c(0.3, 0.4, 0.45))
# Manual cross-validation
best_mse <- Inf
best_params <- list()
for (min_node_size in tune_grid$min.node.size) {
for (sample_fraction in tune_grid$sample.fraction) {
model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = min_node_size,
sample.fraction = sample_fraction
)
predictions <- predict(model)$predictions
mse <- mean((outcome - predictions)^2)
if (mse < best_mse) {
best_mse <- mse
best_params <- list(min.node.size = min_node_size, sample.fraction = sample_fraction)
}
}
}
print(best_params)
# Refit the model with the best parameters
causal_forest_model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = best_params$min.node.size,
sample.fraction = best_params$sample.fraction
)
# Estimate the treatment effects
treatment_effects <- predict(causal_forest_model)$predictions
# Summarize treatment effects
summary(treatment_effects)
# Assess the model's performance using metrics like MSE
mse <- mean((outcome - treatment_effects)^2)
print(mse)
# Load ggplot2
library(ggplot2)
# Plot treatment effects
ggplot(data, aes(x = treatment_effects)) +
geom_histogram(binwidth = 0.05) +
labs(title = "Distribution of Estimated Treatment Effects",
x = "Estimated Treatment Effect",
y = "Frequency")
# Scatter plot of treatment effects vs. actual engagement rate
ggplot(data, aes(x = treatment_effects, y = engagement_rate)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", col = "red") +
labs(title = "Treatment Effects vs. Engagement Rate",
x = "Estimated Treatment Effect",
y = "Engagement Rate")
# Plot treatment effects by user demographics
ggplot(data, aes(x = age, y = treatment_effects, color = gender)) +
geom_point(alpha = 0.5) +
labs(title = "Treatment Effects by Age and Gender",
x = "Age",
y = "Estimated Treatment Effect",
color = "Gender")
# Install required packages
#install.packages("grf")
#install.packages("tidyverse")
#install.packages("caret")
# Load the libraries
library(grf)
library(tidyverse)
library(caret)
# Generating synthetic data
set.seed(123)
n <- 1000
data <- tibble(
user_id = 1:n,
age = round(runif(n, 18, 65)),
gender = sample(c("Male", "Female"), n, replace = TRUE),
location = sample(c("Urban", "Suburban", "Rural"), n, replace = TRUE),
previous_engagement = runif(n, 0, 1),
treatment = sample(c(0, 1), n, replace = TRUE),
engagement_rate = NA
)
# Generate engagement_rate based on treatment and covariates
data <- data %>%
mutate(
engagement_rate = 0.3 * age / 100 +
0.5 * ifelse(gender == "Female", 1, 0) +
0.4 * ifelse(location == "Urban", 1, 0) +
0.2 * previous_engagement +
0.6 * treatment +
rnorm(n, 0, 0.1)
)
# Preview the data
head(data)
# Handling outliers (example: cap outliers at the 99th percentile)
quantiles <- quantile(data$engagement_rate, probs = c(0.01, 0.99))
data$engagement_rate <- pmin(pmax(data$engagement_rate, quantiles[1]), quantiles[2])
### 4.3 Building and Tuning Causal Forest Models
# Define the variables
covariates <- data %>% select(age, gender, location, previous_engagement)
treatment <- data$treatment
outcome <- data$engagement_rate
# Convert categorical variables to one-hot encoding
covariates_matrix <- model.matrix(~. - 1, data = covariates)
# Fit the causal forest model
causal_forest_model <- causal_forest(as.matrix(covariates_matrix), outcome, treatment)
# Summary of the model
summary(causal_forest_model)
# Define a grid of hyperparameters with sampling fraction less than 0.5
tune_grid <- expand.grid(min.node.size = c(5, 10, 15),
sample.fraction = c(0.3, 0.4, 0.45))
# Manual cross-validation
best_rmse <- Inf
best_params <- list()
for (min_node_size in tune_grid$min.node.size) {
for (sample_fraction in tune_grid$sample.fraction) {
model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = min_node_size,
sample.fraction = sample_fraction
)
predictions <- predict(model)$predictions
rmse <- sqrt(mean((outcome - predictions)^2))
if (rmse < best_rmse) {
best_rmse <- rmse
best_params <- list(min.node.size = min_node_size, sample.fraction = sample_fraction)
}
}
}
print(best_params)
# Refit the model with the best parameters
causal_forest_model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = best_params$min.node.size,
sample.fraction = best_params$sample.fraction
)
# Estimate the treatment effects
treatment_effects <- predict(causal_forest_model)$predictions
# Summarize treatment effects
summary(treatment_effects)
# Assess the model's performance using RMSE
rmse <- sqrt(mean((outcome - treatment_effects)^2))
print(rmse)
# Load ggplot2
library(ggplot2)
# Plot treatment effects
ggplot(data, aes(x = treatment_effects)) +
geom_histogram(binwidth = 0.05) +
labs(title = "Distribution of Estimated Treatment Effects",
x = "Estimated Treatment Effect",
y = "Frequency")
# Scatter plot of treatment effects vs. actual engagement rate
ggplot(data, aes(x = treatment_effects, y = engagement_rate)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", col = "red") +
labs(title = "Treatment Effects vs. Engagement Rate",
x = "Estimated Treatment Effect",
y = "Engagement Rate")
# Plot treatment effects by user demographics
ggplot(data, aes(x = age, y = treatment_effects, color = gender)) +
geom_point(alpha = 0.5) +
labs(title = "Treatment Effects by Age and Gender",
x = "Age",
y = "Estimated Treatment Effect",
color = "Gender")
plot_balance(causal_forest_model)
# Install required packages
#install.packages("grf")
#install.packages("tidyverse")
#install.packages("caret")
# Load the libraries
library(grf)
library(tidyverse)
library(caret)
# Generating synthetic data
set.seed(123)
n <- 1000
data <- tibble(
user_id = 1:n,
age = round(runif(n, 18, 65)),
gender = sample(c("Male", "Female"), n, replace = TRUE),
location = sample(c("Urban", "Suburban", "Rural"), n, replace = TRUE),
previous_engagement = runif(n, 0, 1),
treatment = sample(c(0, 1), n, replace = TRUE),
engagement_rate = NA
)
# Generate engagement_rate based on treatment and covariates
data <- data %>%
mutate(
engagement_rate = 0.3 * age / 100 +
0.5 * ifelse(gender == "Female", 1, 0) +
0.4 * ifelse(location == "Urban", 1, 0) +
0.2 * previous_engagement +
0.6 * treatment +
rnorm(n, 0, 0.1)
)
# Preview the data
head(data)
# Handling outliers (example: cap outliers at the 99th percentile)
quantiles <- quantile(data$engagement_rate, probs = c(0.01, 0.99))
data$engagement_rate <- pmin(pmax(data$engagement_rate, quantiles[1]), quantiles[2])
### 4.3 Building and Tuning Causal Forest Models
# Define the variables
covariates <- data %>% select(age, gender, location, previous_engagement)
treatment <- data$treatment
outcome <- data$engagement_rate
# Convert categorical variables to one-hot encoding
covariates_matrix <- model.matrix(~. - 1, data = covariates)
# Fit the causal forest model
causal_forest_model <- causal_forest(as.matrix(covariates_matrix), outcome, treatment)
# Summary of the model
summary(causal_forest_model)
# Define a grid of hyperparameters with sampling fraction less than 0.5
tune_grid <- expand.grid(min.node.size = c(5, 10, 15),
sample.fraction = c(0.3, 0.4, 0.45))
# Manual cross-validation
best_rmse <- Inf
best_params <- list()
for (min_node_size in tune_grid$min.node.size) {
for (sample_fraction in tune_grid$sample.fraction) {
model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = min_node_size,
sample.fraction = sample_fraction
)
predictions <- predict(model)$predictions
rmse <- sqrt(mean((outcome - predictions)^2))
if (rmse < best_rmse) {
best_rmse <- rmse
best_params <- list(min.node.size = min_node_size, sample.fraction = sample_fraction)
}
}
}
print(best_params)
# Refit the model with the best parameters
causal_forest_model <- causal_forest(
as.matrix(covariates_matrix), outcome, treatment,
min.node.size = best_params$min.node.size,
sample.fraction = best_params$sample.fraction
)
# Estimate the treatment effects
treatment_effects <- predict(causal_forest_model)$predictions
# Summarize treatment effects
summary(treatment_effects)
# Assess the model's performance using RMSE
rmse <- sqrt(mean((outcome - treatment_effects)^2))
print(rmse)
# Load ggplot2
library(ggplot2)
# Plot treatment effects
ggplot(data, aes(x = treatment_effects)) +
geom_histogram(binwidth = 0.05) +
labs(title = "Distribution of Estimated Treatment Effects",
x = "Estimated Treatment Effect",
y = "Frequency")
# Scatter plot of treatment effects vs. actual engagement rate
ggplot(data, aes(x = treatment_effects, y = engagement_rate)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", col = "red") +
labs(title = "Treatment Effects vs. Engagement Rate",
x = "Estimated Treatment Effect",
y = "Engagement Rate")
# Plot treatment effects by user demographics
ggplot(data, aes(x = age, y = treatment_effects, color = gender)) +
geom_point(alpha = 0.5) +
labs(title = "Treatment Effects by Age and Gender",
x = "Age",
y = "Estimated Treatment Effect",
color = "Gender")
# Install the grf package
install.packages("grf")
# Load the grf package
library(grf)
# Set seed for reproducibility
set.seed(42)
# Number of students
n <- 1000
# Simulate student characteristics (covariates)
X <- matrix(rnorm(n * 5), nrow = n, ncol = 5)
colnames(X) <- c("PriorGrades", "Attendance", "SocioeconomicStatus", "InterestInSubject", "ParentalSupport")
# Simulate treatment assignment (1 if treated, 0 if control)
T <- rbinom(n, 1, 0.5)
# Simulate outcomes with a heterogeneous treatment effect
Y <- 5 + X[,1] + 2*X[,2] + 0.5*X[,3] + rnorm(n) + T * (2 + 0.5*X[,1])
# Combine into a data frame for convenience
data <- data.frame(Y, T, X)
# Train a causal forest
causal_forest <- causal_forest(X, Y, T)
# Print a summary of the causal forest
summary(causal_forest)
# Estimate treatment effects
tau_hat <- predict(causal_forest)$predictions
# Add the estimated treatment effects to the data frame
data$tau_hat <- tau_hat
# View the first few rows of the data frame
head(data)
# Split the data into training and estimation sets
train_indices <- sample(1:n, size = n/2)
est_indices <- setdiff(1:n, train_indices)
# Train a causal forest on the training set
causal_forest_train <- causal_forest(X[train_indices, ], Y[train_indices], T[train_indices])
# Estimate treatment effects on the estimation set
tau_hat_est <- predict(causal_forest_train, newdata = X[est_indices, ])$predictions
# Combine the estimation set with the estimated treatment effects
data_est <- data[est_indices, ]
data_est$tau_hat_est <- tau_hat_est
# View the first few rows of the estimation set with the estimated treatment effects
head(data_est)
# Calculate the average treatment effect (ATE) bias
true_ate <- mean((Y[est_indices][T[est_indices] == 1]) - (Y[est_indices][T[est_indices] == 0]))
est_ate <- mean(tau_hat_est)
ate_bias <- est_ate - true_ate
print(paste("Average Treatment Effect (ATE) Bias:", ate_bias))
# Calculate the CATE accuracy using RMSE
true_cate <- 2 + 0.5 * X[est_indices, 1]  # True CATE based on the data generation process
cate_rmse <- sqrt(mean((tau_hat_est - true_cate)^2))
print(paste("Conditional Average Treatment Effect (CATE) RMSE:", cate_rmse))
# Construct confidence intervals and evaluate coverage
alpha <- 0.05
ci <- predict(causal_forest_train, newdata = X[est_indices, ], alpha = alpha)
ci_low <- ci$predictions - ci$variance.estimates
ci_high <- ci$predictions + ci$variance.estimates
coverage <- mean((true_cate >= ci_low) & (true_cate <= ci_high))
print(paste("Coverage of", (1 - alpha) * 100, "% confidence intervals:", coverage))
# Visualize the estimated treatment effects across covariate values
plot(X[est_indices, 1], tau_hat_est, xlab = "PriorGrades", ylab = "Estimated Treatment Effect")
abline(h = 0, col = "red", lty = 2)  # Add a reference line at y = 0
# Extract the leaf indices
leaf_indices <- predict(causal_forest, type = "leaf.index")$leaf.index
# Add the leaf indices to the data frame
data$leaf <- leaf_indices
# Check balance within each leaf
balance_check <- function(leaf) {
leaf_data <- data[data$leaf == leaf, ]
treated <- leaf_data[leaf_data$T == 1, ]
control <- leaf_data[leaf_data$T == 0, ]
if (nrow(treated) > 1 & nrow(control) > 1) {
balance <- sapply(leaf_data[, 3:7], function(col) {
t.test(col ~ leaf_data$T)$p.value
})
return(balance)
} else {
return(rep(NA, 5)) # Return NA if not enough data for t-test
}
}
# Apply balance check to each leaf
balance_results <- lapply(unique(data$leaf), balance_check)
# Print the balance results for the first few leaves
balance_results[1:5]
# Debugging: Print the first few leaves and their data
for (i in 1:5) {
cat("Leaf:", unique(data$leaf)[i], "\n")
print(data[data$leaf == unique(data$leaf)[i], ])
}
install.packages("grf")
install.packages("MatchIt")
install.packages("ggplot2")
# get current work directory
getwd()
#set work directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Simulating a dataset
set.seed(123)  # Setting a seed for reproducibility
foldername = "./data/"
students <- read.table(file = paste(foldername, "student_data.csv"), sep = ",", header = TRUE)
# convert variables to factor
students$noise_level_pre <- as.factor(students$noise_level_pre)
students$noise_level_post <- as.factor(students$noise_level_post)
students$part_time_job <- as.factor(students$part_time_job)
students$grade_improvement <- students$post_move_grade - students$pre_move_grade  # Calculating grade improvement
# Get a summary of the data
summary(students)
# View the structure
str(students)
# Check for NA values
sum(is.na(students))
# exploratory data analysis
# visualize
# Basic exploratory data analysis
summary(students)  # Getting a summary of the dataset
plot(students$pre_move_grade, students$post_move_grade,
main="Pre vs. Post Move Grades",
xlab="Pre Move Grades", ylab="Post Move Grades")  # Scatterplot of grades before and after moving
plot(students$noise_level_pre, students$pre_move_grade,
main="Pre Noise Level vs. Pre Move Grades",
xlab="Pre Noise Level ", ylab="Pre Move Grades")
plot(students$noise_level_post, students$post_move_grade,
main="Post Noise Level vs. Post Move Grades",
xlab="Post Noise Level ", ylab="Post Move Grades")
library(ggplot2)
# Histogram for pre-move grades
ggplot(students, aes(x = pre_move_grade)) + geom_histogram(binwidth = 1, fill = "blue", color = "black")
# Histogram for post-move grades
ggplot(students, aes(x = post_move_grade)) + geom_histogram(binwidth = 1, fill = "green", color = "black")
# Scatter plot for study hours vs grade improvement
ggplot(students, aes(x = study_hours, y = grade_improvement)) + geom_point() + geom_smooth(method = "lm")
# Boxplot for noise level pre-move vs grade improvement
ggplot(students, aes(x = noise_level_pre, y = grade_improvement, fill = noise_level_pre)) + geom_boxplot()
# Boxplot for noise level post-move vs grade improvement
ggplot(students, aes(x = noise_level_post, y = grade_improvement, fill = noise_level_post)) + geom_boxplot()
# Correlation Analysis
# Load the corrplot package
install.packages("corrplot")
library(corrplot)
# Calculate correlations
correlations <- cor(students[, sapply(students, is.numeric)])
# Plot the correlations
corrplot(correlations, method = "circle")
#. Saving Your Plots
g <- ggplot(students, aes(x = study_hours, y = grade_improvement)) + geom_point()
ggsave("study_hours_vs_grade_improvement.png", plot = g)
# simple causal technique
# compare means
# T-test to compare means
t_out=t.test(students$pre_move_grade, students$post_move_grade, paired=TRUE)
t_out
# Regression analysis
lm_model <- lm(grade_improvement ~ noise_level_pre + noise_level_post + study_hours + part_time_job + family_income, data=students)
regression_data = summary(lm_model)  # Summarizing the linear model
regression_data
model_summary <- capture.output(regression_data)
writeLines(model_summary,paste(foldername,"regression_data1.txt"))
residuals_output <- capture.output(residuals(lm_model))
writeLines(residuals_output,paste(foldername,"regression_data_resid1.txt"))
# Propensity score matching - requires the 'MatchIt' package
library(MatchIt)
m.out <- matchit(noise_level_post ~ study_hours + part_time_job + family_income, method = "nearest", data = students)
matched_data <- match.data(m.out)  # Getting the matched dataset
# save data
write.table(matched_data, file = paste(foldername,"matched_data1.txt"), row.names = FALSE, sep = "\t")
## visualize the results
# Visualization - requires the 'ggplot2' package
library(ggplot2)
ggplot(students, aes(x=noise_level_pre, y=grade_improvement, color=noise_level_post)) +
geom_boxplot() +
labs(title="Grade Improvement vs. Noise Level", x="Pre-Move Noise Level", y="Grade Improvement")
install.packages("ggplot2")
# Install necessary packages if you haven't already
install.packages("MatchIt")
install.packages("dplyr")
install.packages("ggplot2")
# Load the libraries
library(MatchIt)
library(dplyr)
library(ggplot2)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
data <- read.csv(file = "./data/ecls.csv")
data
# Remove the any columns to anonymize the data
data_anonymized <- data[, !(names(data) %in% c("childid"))]
# Summary statistics for numeric columns
summary_statistics <- data_anonymized %>%
reframe(across(where(is.numeric), ~ list(mean = mean(.x, na.rm = TRUE),
sd = sd(.x, na.rm = TRUE),
min = min(.x, na.rm = TRUE),
max = max(.x, na.rm = TRUE))))
summary_statistics
# Counts  'catholic' columns
catholic_counts <- table(data_anonymized$catholic)
catholic_counts
# Histograms for numeric variables: Age of mother and father, household income, and child's math score
par(mfrow = c(2, 2))
# Mother's age distribution
ggplot(data_anonymized, aes(x = p5hmage)) +
geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
labs(title = "Distribution of Mother's Age", x = "Mother's Age", y = "Frequency") +
theme_minimal()
# Father's age distribution
ggplot(data_anonymized, aes(x = p5hdage)) +
geom_histogram(binwidth = 1, fill = "gold", color = "black") +
labs(title = "Distribution of Father's Age", x = "Father's Age", y = "Frequency") +
theme_minimal()
# Household income distribution
ggplot(data_anonymized, aes(x = w3income)) +
geom_histogram(fill = "lightgreen", color = "black") +
labs(title = "Distribution of Household Income", x = "Household Income", y = "Frequency") +
theme_minimal()
# Child's math score distribution
ggplot(data_anonymized, aes(x = c5r2mtsc)) +
geom_histogram(fill = "salmon", color = "black") +
labs(title = "Distribution of Child's Math Score", x = "Math Score", y = "Frequency") +
theme_minimal()
# Catholic distribution
ggplot(data_anonymized, aes(x = factor(catholic))) +
geom_bar(fill = "lightcoral") +
labs(title = "Distribution of Catholic", x = "Catholic (0 = No, 1 = Yes)", y = "Count") +
theme_minimal()
# View the first few rows of the anonymized dataframe
head(data_anonymized)
data_anonymized %>%
group_by(catholic) %>%
summarise(n_students = n(),
mean_math = mean(c5r2mtsc_std, na.rm = TRUE),
std_error = sd(c5r2mtsc_std, na.rm = TRUE) / sqrt(n_students))
pre_treatment_covariates <- c('w3income', 'p5numpla', 'w3momed_hsb')
data_anonymized %>%
group_by(catholic) %>%
summarise_at(vars(one_of(pre_treatment_covariates)), ~mean(.x, na.rm = TRUE))
# build the model and update prop score
model_ps <- glm(catholic ~  w3income + p5numpla + w3momed_hsb, family = binomial(), data = data_anonymized)
data_anonymized$pscore <- predict(model_ps, newdata = data_anonymized, type = "response")
#plot the results as a histogram
ggplot(data_anonymized, aes(x = pscore, fill = as.factor(catholic))) +
geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
labs(x = "Propensity Score", y = "Frequency", fill = "Group") +
theme_minimal()
# Remove rows with any NA values in the specified covariates
data_cleaned <- data_anonymized %>%
filter(complete.cases(catholic, race_white, w3income, p5numpla, w3momed_hsb))
# Further remove rows with non-finite values in numeric covariates
numeric_covariates <- c( "w3income", "p5numpla")
data_cleaned <- data_cleaned %>%
filter(sapply(data_cleaned[numeric_covariates], is.finite) %>% rowSums() == length(numeric_covariates))
# run matchit() on data_cleaned
mod_match <- matchit(catholic ~ w3income + p5numpla + w3momed_hsb,
method = "nearest", data = data_cleaned) # or data_imputed
# Using the MatchIt package for visualization
plot(mod_match, type = "jitter")
