data$outcome[is.na(data$outcome)] <- mean_outcome
# Perform t-test after imputation
sensitivity_result <- t.test(outcome ~ treatment, data = data)
print(sensitivity_result)
# Install and load necessary packages
if (!requireNamespace("EValue", quietly = TRUE)) install.packages("EValue")
library(EValue)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 500
# Generate synthetic data
data <- data.frame(
comic_freq = sample(1:5, n, replace = TRUE), # 1: Rarely, 5: Very Frequently
parents_edu = sample(c('Low', 'Medium', 'High'), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
homework_time = rnorm(n, mean = 2, sd = 0.5), # Hours per day
higher_studies = sample(c(0, 1), n, replace = TRUE) # 0: No, 1: Yes
)
# Fit a logistic regression model
model <- glm(higher_studies ~ comic_freq + parents_edu + homework_time,
data = data, family = binomial)
# Print model summary
summary(model)
# Extract odds ratio and confidence interval for comic_freq
or <- exp(coef(model)["comic_freq"])
ci <- exp(confint(model)["comic_freq",])
# Print odds ratio and confidence interval
cat("Odds Ratio for comic_freq:", or, "\n")
cat("95% Confidence Interval:", ci[1], "-", ci[2], "\n")
# Calculate the prevalence of the outcome
outcome_prevalence <- mean(data$higher_studies)
# Function to calculate E-value
calculate_evalue <- function(estimate) {
if (estimate < 1) estimate <- 1 / estimate
return(estimate + sqrt(estimate * (estimate - 1)))
}
# Calculate E-values
evalue_point <- calculate_evalue(or)
evalue_lower <- calculate_evalue(ci[1])
# Print E-values
cat("E-value (point estimate):", evalue_point, "\n")
cat("E-value (lower confidence limit):", evalue_lower, "\n")
# Create data for plotting
conf_strengths <- seq(1, max(evalue_point, evalue_lower) + 0.5, by = 0.1)
bias_factors <- sapply(conf_strengths, function(x) x + sqrt(x * (x-1)))
# Plot
plot(conf_strengths, bias_factors, type = "l",
xlab = "Confounder-Outcome Relative Risk",
ylab = "E-value",
main = "Sensitivity Analysis Plot")
abline(h = or, col = "red", lty = 2)
text(1, or, "Observed OR", pos = 3, col = "red")
abline(h = evalue_point, col = "blue", lty = 2)
text(1, evalue_point, "E-value (point estimate)", pos = 3, col = "blue")
abline(h = evalue_lower, col = "green", lty = 2)
text(1, evalue_lower, "E-value (lower CI)", pos = 3, col = "green")
# Install and load necessary packages
# if (!requireNamespace("EValue", quietly = TRUE)) install.packages("EValue")
# library(EValue)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 500
# Generate synthetic data
data <- data.frame(
comic_freq = sample(1:5, n, replace = TRUE), # 1: Rarely, 5: Very Frequently
parents_edu = sample(c('Low', 'Medium', 'High'), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),
homework_time = rnorm(n, mean = 2, sd = 0.5), # Hours per day
higher_studies = sample(c(0, 1), n, replace = TRUE) # 0: No, 1: Yes
)
# Fit a logistic regression model
model <- glm(higher_studies ~ comic_freq + parents_edu + homework_time,
data = data, family = binomial)
# Print model summary
summary(model)
# Extract odds ratio and confidence interval for comic_freq
or <- exp(coef(model)["comic_freq"])
ci <- exp(confint(model)["comic_freq",])
# Print odds ratio and confidence interval
cat("Odds Ratio for comic_freq:", or, "\n")
cat("95% Confidence Interval:", ci[1], "-", ci[2], "\n")
# Calculate the prevalence of the outcome
outcome_prevalence <- mean(data$higher_studies)
# Function to calculate E-value
calculate_evalue <- function(estimate) {
if (estimate < 1) estimate <- 1 / estimate
return(estimate + sqrt(estimate * (estimate - 1)))
}
# Calculate E-values
evalue_point <- calculate_evalue(or)
evalue_lower <- calculate_evalue(ci[1])
# Print E-values
cat("E-value (point estimate):", evalue_point, "\n")
cat("E-value (lower confidence limit):", evalue_lower, "\n")
# Create data for plotting
conf_strengths <- seq(1, max(evalue_point, evalue_lower) + 0.5, by = 0.1)
bias_factors <- sapply(conf_strengths, function(x) x + sqrt(x * (x-1)))
# Plot
plot(conf_strengths, bias_factors, type = "l",
xlab = "Confounder-Outcome Relative Risk",
ylab = "E-value",
main = "Sensitivity Analysis Plot")
abline(h = or, col = "red", lty = 2)
text(1, or, "Observed OR", pos = 3, col = "red")
abline(h = evalue_point, col = "blue", lty = 2)
text(1, evalue_point, "E-value (point estimate)", pos = 3, col = "blue")
abline(h = evalue_lower, col = "green", lty = 2)
text(1, evalue_lower, "E-value (lower CI)", pos = 3, col = "green")
install.packages("mediation")
install.packages("lavaan")
library(mediation)
library(lavaan)
# PREP DATA
set.seed(123) # For reproducibility
n <- 1000 # Number of observations
# Creating synthetic data
data <- data.frame(
Gender = factor(sample(c("Male", "Female"), n, replace = TRUE)),
AgeGroup = factor(sample(c("Under 30", "30-60", "Over 60"), n, replace = TRUE, prob = c(0.2, 0.5, 0.3))),
IncomeLevel = sample(20000:100000, n, replace = TRUE),
PolicyDuration = sample(1:30, n, replace = TRUE),
NumberofClaims = rpois(n, lambda = 2),
CustomerSatisfaction = sample(1:10, n, replace = TRUE),
Termination = factor(sample(c("Yes", "No"), n, replace = TRUE, prob = c(0.3, 0.7)))
)
# Adding binary column for Termination
data$TerminationBinary <- ifelse(data$Termination == "Yes", 1, 0)
# Converting unordered factors to numeric
data$GenderNumeric <- as.numeric(data$Gender)
data$AgeGroupNumeric <- as.numeric(data$AgeGroup)
# CONDUCT MEDIATION ANALYSIS
str(data)
summary(data)
table(data$Gender, data$Termination)
# Mediator model
med.model <- lm(CustomerSatisfaction ~ NumberofClaims + GenderNumeric + AgeGroupNumeric + IncomeLevel + PolicyDuration, data = data)
# Outcome model
out.model <- glm(TerminationBinary ~ CustomerSatisfaction + NumberofClaims + GenderNumeric + AgeGroupNumeric + IncomeLevel + PolicyDuration, family = "binomial", data = data)
med.out <- mediate(med.model, out.model, treat = "NumberofClaims", mediator = "CustomerSatisfaction", robustSE = TRUE, sims = 500)
summary(med.out)
# ADVANCED MEDIATION MODELS
# Setting up the model with multiple mediators
model <- '
# Mediation paths
CustomerSatisfaction ~ b1*NumberofClaims
IncomeLevel ~ b2*NumberofClaims
TerminationBinary ~ c1*CustomerSatisfaction + c2*IncomeLevel + c3*NumberofClaims
# Indirect effects
CustomerSatisfactionMediation := b1 * c1
IncomeLevelMediation := b2 * c2
# Total effect
TotalEffect := b1 * c1 + b2 * c2 + c3
'
fit <- sem(model, data = data, missing = "ML", estimator = "MLR", fixed.x = FALSE)
summary(fit, standardized = TRUE, fit.measures = TRUE)
# MEDIATION IN THE PRESENCE OF MODERATION
model_modmed <- '
# Moderated mediation paths
CustomerSatisfaction ~ b1*NumberofClaims + b3*GenderNumeric + b4*(NumberofClaims*GenderNumeric)
IncomeLevel ~ b2*NumberofClaims + b5*GenderNumeric + b6*(NumberofClaims*GenderNumeric)
TerminationBinary ~ c1*CustomerSatisfaction + c2*IncomeLevel + c3*NumberofClaims + c4*GenderNumeric
'
fit_modmed <- sem(model_modmed, data = data, missing = "ML", estimator = "MLR", fixed.x = FALSE)
install.packages("mediation")
install.packages("lavaan")
library(mediation)
library(lavaan)
# PREP DATA
set.seed(123) # For reproducibility
n <- 1000 # Number of observations
# Creating synthetic data
data <- data.frame(
Gender = factor(sample(c("Male", "Female"), n, replace = TRUE)),
AgeGroup = factor(sample(c("Under 30", "30-60", "Over 60"), n, replace = TRUE, prob = c(0.2, 0.5, 0.3))),
IncomeLevel = sample(20000:100000, n, replace = TRUE),
PolicyDuration = sample(1:30, n, replace = TRUE),
NumberofClaims = rpois(n, lambda = 2),
CustomerSatisfaction = sample(1:10, n, replace = TRUE),
Termination = factor(sample(c("Yes", "No"), n, replace = TRUE, prob = c(0.3, 0.7)))
)
# Adding binary column for Termination
data$TerminationBinary <- ifelse(data$Termination == "Yes", 1, 0)
# Converting unordered factors to numeric
data$GenderNumeric <- as.numeric(data$Gender)
data$AgeGroupNumeric <- as.numeric(data$AgeGroup)
# CONDUCT MEDIATION ANALYSIS
str(data)
summary(data)
table(data$Gender, data$Termination)
# Mediator model
med.model <- lm(CustomerSatisfaction ~ NumberofClaims + GenderNumeric + AgeGroupNumeric + IncomeLevel + PolicyDuration, data = data)
# Outcome model
out.model <- glm(TerminationBinary ~ CustomerSatisfaction + NumberofClaims + GenderNumeric + AgeGroupNumeric + IncomeLevel + PolicyDuration, family = "binomial", data = data)
med.out <- mediate(med.model, out.model, treat = "NumberofClaims", mediator = "CustomerSatisfaction", robustSE = TRUE, sims = 500)
summary(med.out)
# ADVANCED MEDIATION MODELS
# Setting up the model with multiple mediators
model <- '
# Mediation paths
CustomerSatisfaction ~ b1*NumberofClaims
IncomeLevel ~ b2*NumberofClaims
TerminationBinary ~ c1*CustomerSatisfaction + c2*IncomeLevel + c3*NumberofClaims
# Indirect effects
CustomerSatisfactionMediation := b1 * c1
IncomeLevelMediation := b2 * c2
# Total effect
TotalEffect := b1 * c1 + b2 * c2 + c3
'
fit <- sem(model, data = data, missing = "ML", estimator = "MLR", fixed.x = FALSE)
summary(fit, standardized = TRUE, fit.measures = TRUE)
# MEDIATION IN THE PRESENCE OF MODERATION
model_modmed <- '
# Moderated mediation paths
CustomerSatisfaction ~ b1*NumberofClaims + b3*GenderNumeric + b4*NumberofClaims:GenderNumeric
IncomeLevel ~ b2*NumberofClaims + b5*GenderNumeric + b6*NumberofClaims:GenderNumeric
TerminationBinary ~ c1*CustomerSatisfaction + c2*IncomeLevel + c3*NumberofClaims + c4*GenderNumeric
'
fit_modmed <- sem(model_modmed, data = data, missing = "ML", estimator = "MLR", fixed.x = FALSE)
summary(fit_modmed, standardized = TRUE, fit.measures = TRUE)
# LONGITUDINAL MEDIATION ANALYSIS
# Extending the dataset for a longitudinal perspective
data$Year <- rep(1:5, each = n/5)
# Building a simple longitudinal mediation model for demonstration
# Assuming CustomerSatisfaction as the mediator for Year 1 impact on TerminationBinary in Year 5
model_long <- '
TerminationBinary ~ a*CustomerSatisfaction + b*NumberofClaims + c*Year
CustomerSatisfaction ~ d*NumberofClaims + e*Year
'
fit_long <- growth(model_long, data = data, estimator = "MLR")
summary(fit_long, standardized = TRUE, fit.measures = TRUE)
install.packages("lavaan")
install.packages("lavaan")
# Step 1: Install Necessary Packages
packages <- c("tidyverse", "caret", "MatchIt", "panelMatch", "ggplot2", "synthpop", "fixest", "dplyr", "lubridate")
install.packages(setdiff(packages, rownames(installed.packages())))
library(tidyverse)
library(MatchIt)
library(fixest)
library(lubridate)
# Step 2: Generate Synthetic Data
set.seed(123)
n <- 1000
data <- tibble(
id = 1:n,
age = sample(18:65, n, replace = TRUE),
biking_purpose = sample(c("sports", "commute", "heavy_carrying", "occasional", "city", "rural"), n, replace = TRUE, prob = c(0.2, 0.3, 0.1, 0.2, 0.1, 0.1)),
weather_condition = sample(c("sunny", "rainy", "windy"), n, replace = TRUE),
road_condition = sample(c("good", "moderate", "poor"), n, replace = TRUE),
price_sensitivity = sample(1:5, n, replace = TRUE),
treatment = sample(0:1, n, replace = TRUE)
)
data$sales = with(data, 200 + 20 * treatment - 5 * price_sensitivity + ifelse(biking_purpose == "commute", 30, 0) + ifelse(weather_condition == "sunny", 15, -10) + rnorm(n, 0, 50))
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal()
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 4: Causal Inference Analysis
# 4.1 Matching for Causal Inference
matchit_model <- matchit(treatment ~ age + biking_purpose + weather_condition + road_condition + price_sensitivity, data = data, method = "nearest")
matched_data <- match.data(matchit_model)
# 4.2 Estimating Average Treatment Effect (ATE)
ate_model <- feols(sales ~ treatment, data = matched_data)
summary(ate_model)
# Optional: Advanced Causal Inference Techniques
ife_model <- feols(sales ~ treatment | age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
summary(ife_model)
# Optional: Advanced Causal Inference Techniques
ife_model <- feols(sales ~ treatment | age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
summary(ate_model)
# Optional: Advanced Causal Inference Techniques
ife_model <- feols(sales ~ treatment | age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
for fixed effects model
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
# Step 1: Install Necessary Packages
packages <- c("tidyverse", "caret", "MatchIt", "panelMatch", "ggplot2", "synthpop", "fixest", "dplyr", "lubridate")
install.packages(setdiff(packages, rownames(installed.packages())))
library(tidyverse)
library(MatchIt)
library(fixest)
library(lubridate)
# Step 2: Generate Synthetic Data
set.seed(100)
n <- 1000
data <- tibble(
id = 1:n,
age = sample(18:65, n, replace = TRUE),
biking_purpose = sample(c("sports", "commute", "heavy_carrying", "occasional", "city", "rural"), n, replace = TRUE, prob = c(0.2, 0.3, 0.1, 0.2, 0.1, 0.1)),
weather_condition = sample(c("sunny", "rainy", "windy"), n, replace = TRUE),
road_condition = sample(c("good", "moderate", "poor"), n, replace = TRUE),
price_sensitivity = sample(1:5, n, replace = TRUE),
treatment = sample(0:1, n, replace = TRUE)
)
data$sales = with(data, 200 + 20 * treatment - 5 * price_sensitivity + ifelse(biking_purpose == "commute", 30, 0) + ifelse(weather_condition == "sunny", 15, -10) + rnorm(n, 0, 50))
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal()
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 4: Causal Inference Analysis
# 4.1 Matching for Causal Inference
matchit_model <- matchit(treatment ~ age + biking_purpose + weather_condition + road_condition + price_sensitivity, data = data, method = "nearest")
matched_data <- match.data(matchit_model)
# 4.2 Estimating Average Treatment Effect (ATE)
ate_model <- feols(sales ~ treatment, data = matched_data)
summary(ate_model)
# Optional: Advanced Causal Inference Techniques
ife_model <- feols(sales ~ treatment | age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
# Using 'fixest' for two-way fixed effects model (e.g., individual and time fixed effects)
fe_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id + year, data = data)
summary(fe_model)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
lot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal()
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal()
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 1: Install Necessary Packages
packages <- c("tidyverse", "caret", "MatchIt", "panelMatch", "ggplot2", "synthpop", "fixest", "dplyr", "lubridate")
install.packages(setdiff(packages, rownames(installed.packages())))
library(tidyverse)
library(MatchIt)
library(fixest)
library(lubridate)
# Step 2: Generate Synthetic Data
set.seed(123)
n <- 1000
time_periods <- 5
data <- tibble(
id = rep(1:(n/time_periods), each = time_periods),
year = rep(2019:2023, n/time_periods),
age = sample(18:65, n, replace = TRUE),
biking_purpose = sample(c("sports", "commute", "heavy_carrying", "occasional", "city", "rural"), n, replace = TRUE, prob = c(0.2, 0.3, 0.1, 0.2, 0.1, 0.1)),
weather_condition = sample(c("sunny", "rainy", "windy"), n, replace = TRUE),
road_condition = sample(c("good", "moderate", "poor"), n, replace = TRUE),
price_sensitivity = sample(1:5, n, replace = TRUE),
treatment = sample(0:1, n, replace = TRUE)
)
data$sales = with(data, 200 + 20 * treatment - 5 * price_sensitivity + ifelse(biking_purpose == "commute", 30, 0) + ifelse(weather_condition == "sunny", 15, -10) + rnorm(n, 0, 50))
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 4: Causal Inference Analysis
# 4.1 Matching for Causal Inference
matchit_model <- matchit(treatment ~ age + biking_purpose + weather_condition + road_condition + price_sensitivity, data = data, method = "nearest")
matched_data <- match.data(matchit_model)
# 4.2 Estimating Average Treatment Effect (ATE)
ate_model <- feols(sales ~ treatment, data = matched_data)
summary(ate_model)
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
summary(ife_model)
# Using 'fixest' for two-way fixed effects model (individual and time fixed effects)
fe_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id + year, data = data)
summary(fe_model)
# Using 'fixest' for two-way fixed effects model (individual and time fixed effects)
fe_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id + year, data = data)
summary(fe_model)
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
# Correct syntax for fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
summary(ife_model)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 1: Install Necessary Packages
packages <- c("tidyverse", "caret", "MatchIt", "panelMatch", "ggplot2", "synthpop", "fixest", "dplyr", "lubridate")
install.packages(setdiff(packages, rownames(installed.packages())))
library(tidyverse)
library(MatchIt)
library(fixest)
library(lubridate)
# Step 2: Generate Synthetic Data
set.seed(123)
n <- 1000
time_periods <- 5
data <- tibble(
id = rep(1:(n/time_periods), each = time_periods),
year = rep(2019:2023, n/time_periods),
age = sample(18:65, n, replace = TRUE),
biking_purpose = sample(c("sports", "commute", "heavy_carrying", "occasional", "city", "rural"), n, replace = TRUE, prob = c(0.2, 0.3, 0.1, 0.2, 0.1, 0.1)),
weather_condition = sample(c("sunny", "rainy", "windy"), n, replace = TRUE),
road_condition = sample(c("good", "moderate", "poor"), n, replace = TRUE),
price_sensitivity = sample(1:5, n, replace = TRUE),
treatment = sample(0:1, n, replace = TRUE)
)
data$sales = with(data, 200 + 20 * treatment - 5 * price_sensitivity + ifelse(biking_purpose == "commute", 30, 0) + ifelse(weather_condition == "sunny", 15, -10) + rnorm(n, 0, 50))
# Step 3: Exploratory Data Analysis (EDA)
ggplot(data, aes(x = biking_purpose, fill = as.factor(treatment))) +
geom_bar(position = "dodge") +
labs(title = "Biking Purpose by Treatment", x = "Biking Purpose", y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
plot.margin = unit(c(1, 1, 1, 1), "cm")
)
# Step 4: Causal Inference Analysis
# 4.1 Matching for Causal Inference
matchit_model <- matchit(treatment ~ age + biking_purpose + weather_condition + road_condition + price_sensitivity, data = data, method = "nearest")
matched_data <- match.data(matchit_model)
# 4.2 Estimating Average Treatment Effect (ATE)
ate_model <- feols(sales ~ treatment, data = matched_data)
summary(ate_model)
# Using 'fixest' for two-way fixed effects model (individual and time fixed effects)
fe_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id + year, data = data)
summary(fe_model)
# fixed effects model
ife_model <- feols(sales ~ treatment + age + biking_purpose + weather_condition + road_condition + price_sensitivity | id, data = data)
summary(ife_model)
library(dagitty)
library(lavaan)
# load the data set
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
social_media_data = read.csv("./data/social_media_data.csv")
# Define the causal graph
graph <- dagitty("dag {
userA_numfollowers -> num_post_likes
userA_numfollowers -> num_post_commented
userA_numposts -> num_post_likes
userA_numposts -> num_post_commented
interests -> num_post_likes
interests -> num_post_commented
hours_active_perday -> num_post_likes
hours_active_perday -> num_post_commented
}")
plot(graph, cex = 4.5)
# Prepare the data for analysis
vars_in_model <- c("userA_numfollowers", "userA_numposts", "interests", "hours_active_perday", "num_post_likes", "num_post_commented")
corr <- lavCor(social_media_data[vars_in_model])
# Perform local tests using the corrected covariance matrix
local_test_results <- localTests(graph, sample.cov = corr, sample.nobs = nrow(social_media_data))
plotLocalTestResults(local_test_results)
local_test_results
library(dagitty)
library(lavaan)
# load the data set
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
social_media_data = read.csv("./data/social_media_data.csv")
# Define the causal graph
graph <- dagitty("dag {
userA_numfollowers -> num_post_likes
userA_numfollowers -> num_post_commented
userA_numposts -> num_post_likes
userA_numposts -> num_post_commented
interests -> num_post_likes
interests -> num_post_commented
hours_active_perday -> num_post_likes
hours_active_perday -> num_post_commented
}")
plot(graph, cex = 4.5)
# Prepare the data for analysis
vars_in_model <- c("userA_numfollowers", "userA_numposts", "interests", "hours_active_perday", "num_post_likes", "num_post_commented")
corr <- lavCor(social_media_data[vars_in_model])
# Perform local tests using the corrected covariance matrix
local_test_results <- localTests(graph, sample.cov = corr, sample.nobs = nrow(social_media_data))
plotLocalTestResults(local_test_results)
local_test_results
plot(graph, cex = 4.5)
plotLocalTestResults(local_test_results)
